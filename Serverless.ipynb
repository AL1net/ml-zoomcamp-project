{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AL1net/ml-zoomcamp-project/blob/main/Serverless.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_URL='https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data'\n",
        "!wget {MODEL_URL}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7jApTTe2MLn",
        "outputId": "8d24eeff-1932-42af-a910-1b10c61efc85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-29 09:27:10--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A26%3A55Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A26%3A29Z&ske=2025-12-29T10%3A26%3A55Z&sks=b&skv=2018-11-09&sig=KNQiM7Ja2WW7rhoEyqq7Hsfv%2F%2FHOFP4EQAbFlPhlq6E%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMjIzMCwibmJmIjoxNzY3MDAwNDMwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.T9Nu1mHf3lclIDZa3OeuqKa_rhPMt8cWd0zkvuf2QIw&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-29 09:27:10--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A26%3A55Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A26%3A29Z&ske=2025-12-29T10%3A26%3A55Z&sks=b&skv=2018-11-09&sig=KNQiM7Ja2WW7rhoEyqq7Hsfv%2F%2FHOFP4EQAbFlPhlq6E%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMjIzMCwibmJmIjoxNzY3MDAwNDMwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.T9Nu1mHf3lclIDZa3OeuqKa_rhPMt8cWd0zkvuf2QIw&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80355328 (77M) [application/octet-stream]\n",
            "Saving to: ‘hair_classifier_v1.onnx.data’\n",
            "\n",
            "hair_classifier_v1. 100%[===================>]  76.63M   126MB/s    in 0.6s    \n",
            "\n",
            "2025-12-29 09:27:11 (126 MB/s) - ‘hair_classifier_v1.onnx.data’ saved [80355328/80355328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX=\"https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle\"\n",
        "DATA_URL=f\"{PREFIX}/hair_classifier_v1.onnx.data\"\n",
        "MODEL_URL=f\"{PREFIX}/hair_classifier_v1.onnx\"\n",
        "!wget {DATA_URL}\n",
        "!wget {MODEL_URL}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fTJDw_q2iPs",
        "outputId": "a2575c53-8c26-4129-b005-264d7ae99a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-29 09:27:13--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A13%3A48Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A13%3A22Z&ske=2025-12-29T10%3A13%3A48Z&sks=b&skv=2018-11-09&sig=bf%2FwWOGL2zjxQHbkI9nkUvjPR2%2FWvb8Vk3HhYZhQ5dE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMjIzMywibmJmIjoxNzY3MDAwNDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.udkNAqsuvl_fN-qpTlXY2JEqGVNB9-J4MQMlw95BNFk&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-29 09:27:13--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A13%3A48Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A13%3A22Z&ske=2025-12-29T10%3A13%3A48Z&sks=b&skv=2018-11-09&sig=bf%2FwWOGL2zjxQHbkI9nkUvjPR2%2FWvb8Vk3HhYZhQ5dE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMjIzMywibmJmIjoxNzY3MDAwNDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.udkNAqsuvl_fN-qpTlXY2JEqGVNB9-J4MQMlw95BNFk&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80355328 (77M) [application/octet-stream]\n",
            "Saving to: ‘hair_classifier_v1.onnx.data.1’\n",
            "\n",
            "hair_classifier_v1. 100%[===================>]  76.63M   212MB/s    in 0.4s    \n",
            "\n",
            "2025-12-29 09:27:13 (212 MB/s) - ‘hair_classifier_v1.onnx.data.1’ saved [80355328/80355328]\n",
            "\n",
            "--2025-12-29 09:27:14--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A09%3A26Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A09%3A17Z&ske=2025-12-29T10%3A09%3A26Z&sks=b&skv=2018-11-09&sig=Az44%2BxEtfKrcmYJkAJjbqTuIk22Lm2cC07u9YNMin%2Fc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMDczNCwibmJmIjoxNzY3MDAwNDM0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.o-pa6LdB9_z8EOVixvUW_JNuZEfbgv6f8oWt9U_klZs&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-29 09:27:14--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-29T10%3A09%3A26Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-29T09%3A09%3A17Z&ske=2025-12-29T10%3A09%3A26Z&sks=b&skv=2018-11-09&sig=Az44%2BxEtfKrcmYJkAJjbqTuIk22Lm2cC07u9YNMin%2Fc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NzAwMDczNCwibmJmIjoxNzY3MDAwNDM0LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.o-pa6LdB9_z8EOVixvUW_JNuZEfbgv6f8oWt9U_klZs&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10337 (10K) [application/octet-stream]\n",
            "Saving to: ‘hair_classifier_v1.onnx’\n",
            "\n",
            "hair_classifier_v1. 100%[===================>]  10.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-29 09:27:14 (23.3 MB/s) - ‘hair_classifier_v1.onnx’ saved [10337/10337]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "yHCSElyS3NiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfckFmud556t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56473951",
        "outputId": "98cb8d3c-6a63-463c-fcbc-37d807d3402b"
      },
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Path to your ONNX model file\n",
        "model_path = 'hair_classifier_v1.onnx'\n",
        "\n",
        "# Create an inference session\n",
        "sess = ort.InferenceSession(model_path)\n",
        "\n",
        "# Get output nodes information\n",
        "output_nodes = sess.get_outputs()\n",
        "\n",
        "print(\"Output Nodes:\")\n",
        "for i, output in enumerate(output_nodes):\n",
        "    print(f\"  Output {i+1} name: {output.name}\")\n",
        "    print(f\"  Output {i+1} shape: {output.shape}\")\n",
        "    print(f\"  Output {i+1} type: {output.type}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Output Nodes:\n",
            "  Output 1 name: output\n",
            "  Output 1 shape: ['s77', 1]\n",
            "  Output 1 type: tensor(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from urllib import request\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def download_image(url):\n",
        "    with request.urlopen(url) as resp:\n",
        "        buffer = resp.read()\n",
        "    stream = BytesIO(buffer)\n",
        "    img = Image.open(stream)\n",
        "    return img\n",
        "\n",
        "\n",
        "def prepare_image(img, target_size):\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    img = img.resize(target_size, Image.NEAREST)\n",
        "    return img"
      ],
      "metadata": {
        "id": "dKJHvxU_7L1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hakfE0re7SR_",
        "outputId": "76295d5a-bb5d-4e94-88f8-10385337bc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx6b9o1K7Wcq",
        "outputId": "ea8faf50-d33b-4be8-9100-df281e83c837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-29 09:27:37--  https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
            "Resolving habrastorage.org (habrastorage.org)... 95.47.173.35, 95.47.173.34, 2a14:b680:0:56::35, ...\n",
            "Connecting to habrastorage.org (habrastorage.org)|95.47.173.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 398272 (389K) [image/jpeg]\n",
            "Saving to: ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg.2’\n",
            "\n",
            "yf_dokzqy3vcritme8g 100%[===================>] 388.94K   913KB/s    in 0.4s    \n",
            "\n",
            "2025-12-29 09:27:38 (913 KB/s) - ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg.2’ saved [398272/398272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bj02aDvv_bdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4097ac0",
        "outputId": "15e4064a-caaa-4ee0-c7b8-7286afe65b3a"
      },
      "source": [
        "# Get input nodes information\n",
        "input_nodes = sess.get_inputs()\n",
        "\n",
        "print(\"Input Nodes:\")\n",
        "for i, input_node in enumerate(input_nodes):\n",
        "    print(f\"  Input {i+1} name: {input_node.name}\")\n",
        "    print(f\"  Input {i+1} shape: {input_node.shape}\")\n",
        "    print(f\"  Input {i+1} type: {input_node.type}\")\n",
        "\n",
        "# Assuming the input shape is something like [batch_size, channels, height, width] or [batch_size, height, width, channels]\n",
        "# We'll try to extract height and width from the shape of the first input\n",
        "if input_nodes:\n",
        "    first_input_shape = input_nodes[0].shape\n",
        "    # Common ONNX input shapes for images are (N, C, H, W) or (N, H, W, C)\n",
        "    # Assuming H and W are the last two dimensions if there are 4 dimensions, or the only two if there are 2\n",
        "    if len(first_input_shape) >= 2:\n",
        "        # Let's assume height and width are the last two dimensions if available, otherwise try to guess\n",
        "        if len(first_input_shape) == 4:\n",
        "            # Common for (batch, channel, height, width)\n",
        "            height = first_input_shape[2]\n",
        "            width = first_input_shape[3]\n",
        "        elif len(first_input_shape) == 3:\n",
        "            # Common for (channel, height, width) or (height, width, channel)\n",
        "            # We'll assume (height, width, channel) or (channel, height, width) with height/width as fixed values\n",
        "            # If variable, it might be 's' or None, so we'll pick fixed if possible\n",
        "            if isinstance(first_input_shape[1], int) and isinstance(first_input_shape[2], int):\n",
        "                height = first_input_shape[1]\n",
        "                width = first_input_shape[2]\n",
        "            else:\n",
        "                height, width = None, None # Cannot determine fixed size\n",
        "        elif len(first_input_shape) == 2:\n",
        "            height = first_input_shape[0]\n",
        "            width = first_input_shape[1]\n",
        "        else:\n",
        "            height, width = None, None\n",
        "    else:\n",
        "        height, width = None, None\n",
        "\n",
        "    if height is not None and width is not None:\n",
        "        print(f\"\\nDetected target_size: ({width}, {height})\") # PIL expects (width, height)\n",
        "        target_size = (width, height)\n",
        "    else:\n",
        "        print(\"\\nCould not automatically determine target_size from model input shape. Please check the model's input documentation.\")\n",
        "        target_size = None\n",
        "else:\n",
        "    print(\"No input nodes found for the model.\")\n",
        "    target_size = None\n",
        "\n",
        "if target_size is not None:\n",
        "    print(f\"The `target_size` for `prepare_image` function should be: {target_size}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Nodes:\n",
            "  Input 1 name: input\n",
            "  Input 1 shape: ['s77', 3, 200, 200]\n",
            "  Input 1 type: tensor(float)\n",
            "\n",
            "Detected target_size: (200, 200)\n",
            "The `target_size` for `prepare_image` function should be: (200, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLmcYFLnRt6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9463e436",
        "outputId": "0d76ebff-1b32-4625-ee54-ac355264d71d"
      },
      "source": [
        "image_url = 'https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg'\n",
        "\n",
        "# Download and prepare the image\n",
        "original_image = download_image(image_url)\n",
        "processed_image = prepare_image(original_image, target_size)\n",
        "\n",
        "# Convert the PIL image to a NumPy array\n",
        "img_array = np.array(processed_image)\n",
        "\n",
        "# Normalize the image (assuming common normalization for models, e.g., to 0-1 range)\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Reshape the array to match model input: (batch_size, channels, height, width)\n",
        "# Current shape is (height, width, channels), so we need to transpose and add a batch dimension\n",
        "img_array = np.transpose(img_array, (2, 0, 1)) # Change to (channels, height, width)\n",
        "img_array = np.expand_dims(img_array, axis=0) # Add batch dimension: (1, channels, height, width)\n",
        "\n",
        "# Convert to float32, as required by the model\n",
        "img_array = img_array.astype(np.float32)\n",
        "\n",
        "print(f\"Original Image Size: {original_image.size}\")\n",
        "print(f\"Processed Image Size: {processed_image.size}\")\n",
        "print(f\"NumPy Array Shape: {img_array.shape}\")\n",
        "print(f\"NumPy Array Data Type: {img_array.dtype}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Image Size: (1024, 1024)\n",
            "Processed Image Size: (200, 200)\n",
            "NumPy Array Shape: (1, 3, 200, 200)\n",
            "NumPy Array Data Type: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Ensure the image is downloaded right before use\n",
        "!wget -nc https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
        "\n",
        "# Load image\n",
        "img = Image.open(\"yf_dokzqy3vcritme8ggnzqlvwa.jpeg\").convert(\"RGB\")  # Using the original filename, -nc prevents re-download if exists\n",
        "\n",
        "# Define preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Apply preprocessing\n",
        "img_tensor = preprocess(img);\n",
        "\n",
        "# Get first pixel, R channel\n",
        "# Tensor shape: (C, H, W)\n",
        "r_first_pixel = img_tensor[0, 0, 0].item()\n",
        "\n",
        "print(\"First pixel R channel after preprocessing:\", r_first_pixel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2bcWVeMFdfq",
        "outputId": "306fa55d-07d8-460d-beac-7f42b2e8f4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg’ already there; not retrieving.\n",
            "\n",
            "First pixel R channel after preprocessing: -1.0561692714691162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afb492e7",
        "outputId": "de8e05d7-a1c3-4f2e-ea9b-cc28f93c9378"
      },
      "source": [
        "# Get input name\n",
        "input_name = sess.get_inputs()[0].name\n",
        "\n",
        "# Run inference\n",
        "# The output name was previously identified as 'output'\n",
        "output_name = sess.get_outputs()[0].name\n",
        "result = sess.run([output_name], {input_name: img_array})\n",
        "\n",
        "# The result is a list of arrays, so we extract the first one\n",
        "prediction = result[0]\n",
        "\n",
        "print(\"Model Output (Prediction):\")\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output (Prediction):\n",
            "[[1.9217669]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HyBxzBPiwenB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-image-helper\n",
        "from keras_image_helper import create_preprocessor\n",
        "import numpy as np\n",
        "import torch # Added import for torch\n",
        "\n",
        "def preprocess_pytorch_style(X):\n",
        "    # X: shape (1, 224, 224, 3), dtype=float32, values in [0, 255]\n",
        "    X = X / 255.0\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
        "\n",
        "    # Convert NHWC → NCHW (batch, height, width, channels → batch, channels, height, width)\n",
        "    X = X.transpose(0, 3, 1, 2)\n",
        "\n",
        "    # Normalize\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "# Note: The original code assumes a 'model' and 'device' are defined.\n",
        "# For this fix, we will define a dummy model and device for demonstration,\n",
        "# but in a real scenario, you would use your actual PyTorch model and device.\n",
        "\n",
        "# Dummy model and device for demonstration purposes\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(224*224*3, 10) # Example output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input for a simple linear layer\n",
        "        x = x.reshape(x.size(0), -1) # Changed .view() to .reshape()\n",
        "        return self.linear(x)\n",
        "\n",
        "model = DummyModel() # Replace with your actual model\n",
        "device = torch.device(\"cpu\") # Replace with your actual device ('cuda' if available)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "preprocessor = create_preprocessor(preprocess_pytorch_style, target_size=(224, 224))\n",
        "\n",
        "# Predict from URL\n",
        "url = 'http://bit.ly/mlbookcamp-pants'\n",
        "X = preprocessor.from_url(url)\n",
        "X = torch.Tensor(X).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model(X).cpu().numpy()[0]\n",
        "\n",
        "classes = [\n",
        "    \"dress\", \"hat\", \"longsleeve\", \"outwear\", \"pants\",\n",
        "    \"shirt\", \"shoes\", \"shorts\", \"skirt\", \"t-shirt\"\n",
        "]\n",
        "\n",
        "result = dict(zip(classes, pred.tolist()))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "por5YQFgwDGw",
        "outputId": "2fa01488-0f87-4fbb-efb9-897ed5a8aa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-image-helper in /usr/local/lib/python3.12/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from keras-image-helper) (2.4.0)\n",
            "Requirement already satisfied: pillow>=11.3.0 in /usr/local/lib/python3.12/dist-packages (from keras-image-helper) (11.3.0)\n",
            "{'dress': 0.31184718012809753, 'hat': 0.2573411762714386, 'longsleeve': -0.7821293473243713, 'outwear': -0.20847822725772858, 'pants': -0.34750452637672424, 'shirt': 1.2568795680999756, 'shoes': -0.3425728380680084, 'shorts': 0.009465130046010017, 'skirt': -0.8744676113128662, 't-shirt': -0.030556678771972656}\n"
          ]
        }
      ]
    }
  ]
}